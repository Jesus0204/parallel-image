# Evidence: 4 Parallel Programming Paradigm
Jesús Alejandro Cedillo Zertuche A01705442

## Context
Since the beginning of computing, the method that has been used is Serial computing. Geeks For Geeks (2021) mentions that an algorithm divides a problem into small instructions that are given to the central processing unit one by one. The important thing to note here is that the next one starts to process only after one instruction is finished. A good comparison is a line where there is only one cashier, so they can only give tickets to one person at a time. This was a big problem since the hardware was being wasted by only using one part of its power. 

This is why parallel computing was thought of as where "problems are broken down into instructions and are solved concurrently as each resource that has been applied to work is working at the same time" (Geeks for Geeks, 2021). 

Now before I explain my project there is something important to note here. In the previous sentence, I used the word concurrent, and Concurrent programming is not the same as Parallel programming. Nowadays, computers have multiple cores or CPUs. Vistorskyte (2021) mentions the following definition: "Concurrency is when multiple tasks can run in overlapping periods. It’s an illusion of multiple tasks running in parallel because of a very fast switching by the CPU. Two tasks can’t run at the same time on a single-core CPU. Parallelism is when tasks actually run in parallel in multiple CPUs [or cores].

This means that concurrency is multiple tasks over a single CPU, while parallel programming is running a single task using several CPUs or cores. 

## References
Geeks for Geeks. (June 4, 2021). Introduction to Parallel Computing. https://www.geeksforgeeks.org/introduction-to-parallel-computing/ 

Vistorskyte, I. (July 1, 2021). Concurrency vs Parallelism: The Main Differences. https://oxylabs.io/blog/concurrency-vs-parallelism
